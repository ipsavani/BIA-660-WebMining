{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>HW 4: Text preprocessing</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, json, string\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import string\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Regular Expression (2 points)\n",
    "\n",
    "Suppose you have scraped the text shown below from an online source. You'd like to extract data using regular expression.\n",
    "\n",
    "Define a **extract** function which:\n",
    "- Takes a piece of text (in the format of shown below) as an input\n",
    "- Extracts data into a list of tuples using regular expression, e.g.  `[('BTC-USD','56,212.15','-58.16','-0.10%'), ('ETH-USD',  ...), ...]`\n",
    "- Returns the list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Symbol   Last Price  Change   % Change   Note\\n                  BTC-USD  56,212.15   -58.16   -0.10%   Bitcoin \\n                  ETH-USD  1,787.79    -53.63   -2.91%   Ether\\n                  BNB-USD  1,101,290.51      +5.81    +2.04%   Binance\\n                  USDT-USD 1.0003      -0.0004  -0.04%   Tether\\n                  ADA-USD  1.1187      -0.0528  -4.51%   Cardano\\n                  \\n                  \\n                  new test row below to show code works for all new values in the same format\\n                  and disregards any other text\\n                  TESTADA-USD  jxnjx 1.1187   1.5%   -0.0528  x% 1%  -4.51%    Cardano\\n                  \\n                  incase of invalid format of actual data ignore entire row \\n                  invalidADA-USD   1.d87  -0.5f -4.51% Cardano\\n      '"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='''Symbol   Last Price  Change   % Change   Note\n",
    "                  BTC-USD  56,212.15   -58.16   -0.10%   Bitcoin \n",
    "                  ETH-USD  1,787.79    -53.63   -2.91%   Ether\n",
    "                  BNB-USD  1,101,290.51      +5.81    +2.04%   Binance\n",
    "                  USDT-USD 1.0003      -0.0004  -0.04%   Tether\n",
    "                  ADA-USD  1.1187      -0.0528  -4.51%   Cardano\n",
    "                  \n",
    "                  \n",
    "                  new test row below to show code works for all new values in the same format\n",
    "                  and disregards any other text\n",
    "                  TESTADA-USD  jxnjx 1.1187   1.5%   -0.0528  x% 1%  -4.51%    Cardano\n",
    "                  \n",
    "                  incase of invalid format of actual data ignore entire row \n",
    "                  invalidADA-USD   1.d87  -0.5f -4.51% Cardano\n",
    "      '''\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "def extract(text):\n",
    "    # add your code\n",
    "\n",
    "    #                           Symbol group      Last price group       Change group      %Change group\n",
    "    tuple_list = re.findall(r'([A-Z]+-[A-Z]+).*\\s(\\d+[,*\\d*]*\\.\\d+)\\s.*([-|+]\\d+\\.\\d+)\\s.*([-|+]\\d+\\.\\d+%)',text)\n",
    "    \n",
    "    return tuple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BTC-USD', '56,212.15', '-58.16', '-0.10%'),\n",
       " ('ETH-USD', '1,787.79', '-53.63', '-2.91%'),\n",
       " ('BNB-USD', '1,101,290.51', '+5.81', '+2.04%'),\n",
       " ('USDT-USD', '1.0003', '-0.0004', '-0.04%'),\n",
       " ('ADA-USD', '1.1187', '-0.0528', '-4.51%'),\n",
       " ('TESTADA-USD', '1.1187', '-0.0528', '-4.51%')]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "\n",
    "extract(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Collocation (3 points)\n",
    "\n",
    "Define a function `top_collocation(doc, K)` to find top-K collocations in specific patterns in a document as follows:\n",
    "  - Takes a document (i.e. `doc`) and `K` as inputs\n",
    "  - Find collocations as follows:\n",
    "    - Tokenize the document and find POS tag of each token (hint: you can use NLTK word tokenizer or Spacy tokenizer).\n",
    "    - Create bigrams from the tokens with POS tags.\n",
    "\n",
    "    - Keep only bigrams matching the following patterns:\n",
    "       - `Adj + Noun`: e.g. linear function\n",
    "       - `Noun + Noun`: e.g. regression coefficient\n",
    "    - Get frequency of each bigram (hint: you can use nltk.FreqDist)\n",
    "    - Returns top K collocations by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "# Define the function\n",
    "\n",
    "\n",
    "def top_collocation(doc, K):\n",
    "    \n",
    "    # add your code\n",
    "    # Custom tokenize words to preserve (-) separated and (') punctuation words.\n",
    "    tokens = nltk.regexp_tokenize(doc, r'\\w[\\w\\',-]*\\w')\n",
    "    \n",
    "    \n",
    "    # Get POS tags for each token\n",
    "    pos_tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Create bigrams from the tokens with POS tags.\n",
    "    bigrams = list(nltk.bigrams(pos_tagged_tokens))\n",
    "    \n",
    "    # Keep only bigrams matching the pattern: Adj + Noun or Noun + Noun\n",
    "    filter_bigrams=[ (x[0],y[0]) for (x,y) in bigrams \\\n",
    "         if (x[1].startswith('JJ') or x[1].startswith('NN'))\\\n",
    "         and y[1].startswith('NN')]\n",
    "    \n",
    "    # Get frequency of each bigram\n",
    "    f_dist = nltk.FreqDist(filter_bigrams)\n",
    "    \n",
    "    # Get top K collocations by frequency\n",
    "    result = f_dist.most_common(K)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('public', 'health'), 14),\n",
       " (('community', 'spread'), 9),\n",
       " (('United', 'States'), 8),\n",
       " (('higher', 'risk'), 4),\n",
       " (('COVID-19', 'illness'), 4),\n",
       " (('elevated', 'risk'), 4),\n",
       " (('new', 'coronavirus'), 3),\n",
       " (('health', 'threat'), 3),\n",
       " (('serious', 'COVID-19'), 3),\n",
       " (('new', 'virus'), 3)]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open(\"qa.json\",\"r\"))\n",
    "article = data[\"context\"]\n",
    "\n",
    "top_collocation(article, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Question and Answering (QA) System (5 points)\n",
    "\n",
    "Develop a QA system which allow you to search for answers in an article. For example, the file `qa.json` contains a research article. This article can answer a number of questions about COVID-19. You will design a solution to automatically search answers to these questions in this article.\n",
    "\n",
    "`qa.json` is taken from https://github.com/deepset-ai/COVID-QA. This file contains a few questions, and answers to these questions have been located in the article. Let's define a QA system and check if your system can locate the right answers.\n",
    "\n",
    "The following script helps you understand `qa.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC Summary 21 MAR 2020,\n",
      "https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/summary.html\n",
      "\n",
      "This is a rapidly evolving situation and CDC will provide updated information and guidance as it becomes \n"
     ]
    }
   ],
   "source": [
    "# Retrieve the article\n",
    "\n",
    "data = json.load(open(\"qa.json\",\"r\"))\n",
    "article = data[\"context\"]\n",
    "\n",
    "# A long article. Just print the first 200 characters\n",
    "print(article[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What age group has the highest rate of severe outcomes?', 'id': 236, 'answers': [{'text': 'people 85 years and older', 'answer_start': 6117}], 'is_impossible': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What age group has the highest rate of severe outcomes?',\n",
       " 'How is COVID-19 spread?',\n",
       " 'How many states in the U.S. have reported cases of COVID-19?',\n",
       " 'When did the White House launch the \"15 Days to Slow the Spread\" program?',\n",
       " 'What should mildly-ill patients do?',\n",
       " 'What type of virus is SARS-CoV-2?',\n",
       " 'What viruses are similar to the COVID-19 coronavirus?',\n",
       " 'What are the phases of a pandemic?',\n",
       " 'At which phase does the peak of the pandemic occur?',\n",
       " 'People with which medical conditions have a higher rate of severe illness?',\n",
       " 'What kind of test can diagnose COVID-19?',\n",
       " 'In what species did the COVID-19 virus likely originate?',\n",
       " 'What risk factors should be considered in addition to clinical symptoms?']"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve all the questions and answers\n",
    "qas = data[\"qas\"]\n",
    "\n",
    "# show the first question-answer pair. Note the answer starts at the 6117th character\n",
    "print(qas[0])\n",
    "\n",
    "# get all questions\n",
    "qs = [item[\"question\"] for item in qas]\n",
    "qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, following the instructions below step by step to develop the QA system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.1. Tokenizer\n",
    "\n",
    "Define a function `tokenize(doc)`  as follows:\n",
    "   - Take a piece of text (i.e. variable `doc`) as an input\n",
    "   - Split the input text into unigrams\n",
    "   - Clean up tokens as follows:\n",
    "       - Lemmatize all unigrams\n",
    "       - Remove all stop words\n",
    "       - Remove all punctuations\n",
    "       - Convert all unigrams to the lower case \n",
    "       - remove empty unigrams\n",
    "   - Return the list of unigrams after all the processing. (Hint: you can use spacy package for this task. To test if a token is stop word or punctuation, check https://spacy.io/api/token#attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "def tokenize(doc):\n",
    "    # add your code\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # tokenize into unigrams\n",
    "    tokens = sp(doc)\n",
    "    \n",
    "    # Lemmatize all unigrams\n",
    "    lemmatized_tokens = [x.lemma_ for x in tokens]\n",
    "    \n",
    "    # Remove all stop words\n",
    "    filt_stop_words = [x for x in lemmatized_tokens if not sp.vocab[x].is_stop]\n",
    "    \n",
    "    # Remove all punctuations\n",
    "    stripped_puncs = [x for x in filt_stop_words if not sp.vocab[x].is_punct]\n",
    "    \n",
    "    # Convert all unigrams to the lower case\n",
    "    lower_tokens = [x.lower() for x in stripped_puncs]\n",
    "    \n",
    "    # Remove empty unigrams\n",
    "    tokens = [x.strip() for x in lower_tokens if x.strip()!='']\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['old', 'people', 'people', 'age', 'severe', 'chronic', 'medical', 'condition', 'like', 'heart', 'disease', 'lung', 'disease', 'diabete', 'example', 'high', 'risk', 'develop', 'covid-19', 'illness']\n"
     ]
    }
   ],
   "source": [
    "doc = 'Older people and people of all ages with severe chronic medical conditions — \\\n",
    "like heart disease, lung disease and diabetes, \\\n",
    "for example — seem to be at higher risk of developing serious COVID-19 illness.'\n",
    "\n",
    "print(tokenize(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2. Compute TF-IDF Matrix\n",
    "\n",
    "Define a function `compute_tfidf(docs)` as follows: \n",
    "\n",
    "- Take `docs`, a list of documents (e.g. a list of questions) as an input\n",
    "- Tokenize each document in `docs` using the `tokenize` function defined in Q3.1. \n",
    "- Calculate tf_idf weights as shown in lecture notes (Hint: you can reuse the last code segment in NLP Lecture Notes (II))\n",
    "- Return a smoothed normalized `tf_idf` array. (The result may differ a little bit depending on the tokenize function and packages you use.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "def compute_tfidf(docs):\n",
    "    # get tokens and convert to dataframe for indexing\n",
    "    docs_tokens = {idx:nltk.FreqDist(tokenize(doc)) for idx,doc in enumerate(docs)}\n",
    "    dtm = pd.DataFrame.from_dict(docs_tokens,orient=\"index\")\n",
    "    # fill empty values and sort by index\n",
    "    dtm = dtm.fillna(0)\n",
    "    dtm = dtm.sort_index(axis = 0)\n",
    "    # Calculate term frequency\n",
    "    tf = dtm.values\n",
    "    doc_len=tf.sum(axis=1)\n",
    "    tf=np.divide(tf, doc_len[:,None])  \n",
    "    df=np.where(tf>0,1,0)\n",
    "    # Calculate transformed Inversed Document Frequency\n",
    "    smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1\n",
    "    # Calculate TF_IDF\n",
    "    smoothed_tf_idf=normalize(tf*smoothed_idf)\n",
    "    return smoothed_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41, 0.41, 0.41, 0.41, 0.41, 0.41, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.61, 0.8 , 0.  , 0.  , 0.  ,\n",
       "        0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.36, 0.  , 0.47, 0.47, 0.47,\n",
       "        0.47]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function using three questions\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "compute_tfidf(qs[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2. Put Everything Together\n",
    "\n",
    "Define a function `find_solutions(qs, article)` as follows: \n",
    "\n",
    "- Take two inputs:\n",
    "    - `qs`: a list of questions (i.e. strings)\n",
    "    - `article`: a document which may contain answers to the questions\n",
    "- Segment the article into sentences (i.e. `sents`). You will locate the sentence which can answer a question.\n",
    "- Concatenate the questions (`qs`) and sentences (`sents`) into a single list (i.e. `qs + sents`)\n",
    "- Call the function `compute_tfidf` defined in Q3.2 with `qs + sents` to get a `TF-IDF` matrix. (Note, now `qs` and `sents` are converted to TF-IDF vectors in the same dimension. As a result, you can measure their similarities.) \n",
    "- Split the `TF-IDF` matrix into two sub matrices, one corresponding to `qs` and the other for `sents`. \n",
    "- Next, calculate the pairwise cosine similarity between the `qs` and `sents`. With $m$ questions and $n$ sentences, you should get a $m \\times n$ matrix. (hint: you can `sklearn.metrics.pairwise_distances` to calculate pairwise distances between two matrices)\n",
    "- Finally, the answer to each question is the sentence which has the `maximum similarity` to it. \n",
    "- Print out each question and its matched answer. Check if your QA system is able to find the right answer.(Depending on the packages you use, your answer might be a bit different from mine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_solutions(qs, article):\n",
    "    \n",
    "    # create a tokenizer using spacy for getting sentences from article\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    tokens = sp(article)\n",
    "    # Get sentences from article by tokens.sents\n",
    "    # Remove new line by string.replace (strip() fails remove new line characters in the middle of a sentence)....\n",
    "    all_sentences = [x.text.replace('\\n',' ') for x in tokens.sents]\n",
    "    # Remove sentences with only whitespaces and only one words to avoid giving importance to low discrimination one words.\n",
    "    sentences = [x for x in all_sentences if not bool(re.search(r'^\\s*\\S+\\s*$|^\\s*$',x))]\n",
    "    # Concatenate the questions and sentences\n",
    "    qs_plus_sentences = qs + sentences\n",
    "    # Get tfidf for the combined list\n",
    "    tf_idf_mat = compute_tfidf(qs_plus_sentences)\n",
    "    # Split tfidf by questions and sentences\n",
    "    tf_idf_split = np.split(tf_idf_mat,[len(qs)])\n",
    "    # Calculate pairwise cosine similarity(both 1-pairwise_distances with metric=cosine and cosine_similarity give same results)\n",
    "    pwcs = 1 - pairwise_distances(tf_idf_split[0],tf_idf_split[1],metric='cosine')\n",
    "    # pwcs = cosine_similarity(tf_idf_split[0],tf_idf_split[1])\n",
    "    \n",
    "    # Match questions with sentences with maximum pairwise cosine similarities\n",
    "    # in case of more than one sentences with max cs value, combine them\n",
    "    results=[['Question: ' + x]+ \\\n",
    "         ['Answers: ' + ','.join([sentences[y] for y in np.where(pwcs[idx]==np.amax(pwcs[idx]))[0]])]  \\\n",
    "         for idx,x in enumerate(qs)]\n",
    "             \n",
    "    return results                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Question: What age group has the highest rate of severe outcomes?',\n",
       "  'Answers: A CDC Morbidity & Mortality Weekly Report that looked at severity of disease among COVID-19 cases in the United States by age group found that 80% of deaths were among adults 65 years and older with the highest percentage of severe outcomes occurring in people 85 years and older.'],\n",
       " ['Question: How is COVID-19 spread?',\n",
       "  'Answers: More than half of the U.S. is seeing some level of community spread of COVID-19.'],\n",
       " ['Question: How many states in the U.S. have reported cases of COVID-19?',\n",
       "  'Answers: All 50 states have reported cases of COVID-19 to CDC.'],\n",
       " ['Question: When did the White House launch the \"15 Days to Slow the Spread\" program?',\n",
       "  'Answers: On March 16, the White House announced a program called “15 Days to Slow the Spread,”pdf iconexternal icon which is a nationwide effort to slow the spread of COVID-19 through the implementation of social distancing at all levels of society.'],\n",
       " ['Question: What should mildly-ill patients do?',\n",
       "  'Answers: People who are mildly ill with COVID-19 are able to isolate at home during their illness.'],\n",
       " ['Question: What type of virus is SARS-CoV-2?',\n",
       "  'Answers: The SARS-CoV-2 virus is a betacoronavirus, like MERS-CoV and SARS-CoV. All three of these viruses have their origins in bats.'],\n",
       " ['Question: What viruses are similar to the COVID-19 coronavirus?',\n",
       "  'Answers: Emergence COVID-19 is caused by a coronavirus.'],\n",
       " ['Question: What are the phases of a pandemic?',\n",
       "  'Answers: Pandemics begin with an investigation phase, followed by recognition, initiation, and acceleration phases.'],\n",
       " ['Question: At which phase does the peak of the pandemic occur?',\n",
       "  'Answers: The peak of illnesses occurs at the end of the acceleration phase, which is followed by a deceleration phase, during which there is a decrease in illnesses.'],\n",
       " ['Question: People with which medical conditions have a higher rate of severe illness?',\n",
       "  'Answers: Older people and people with severe chronic conditions should take special precautions because they are at higher risk of developing serious COVID-19 illness.'],\n",
       " ['Question: What kind of test can diagnose COVID-19?',\n",
       "  'Answers:  CDC developed an rRT-PCR test to diagnose COVID-19.'],\n",
       " ['Question: In what species did the COVID-19 virus likely originate?',\n",
       "  'Answers: Coronaviruses are a large family of viruses that are common in people and many different species of animals, including camels, cattle, cats, and bats.'],\n",
       " ['Question: What risk factors should be considered in addition to clinical symptoms?',\n",
       "  'Answers: Factors to consider in addition to clinical symptoms may include: Does the patient have recent travel from an affected area?']]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_solutions(qs, article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllibs",
   "language": "python",
   "name": "mllibs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
